Extension Plan for CWV-Interactive Platform

This report outlines a comprehensive plan to extend the cwv-interactive repository into a fully interactive user platform. Five key feature areas are addressed, each with defined scope, priority features, implementation steps, and technical considerations. The goal is to enhance the platform with comparison interfaces, engaging user interactions, research paper access, data downloads, and robust user authentication. Best practices with Next.js and integration with model APIs (like OpenAI Codex/GPT) and Supabase are emphasized throughout.

1. Model vs Human Comparison Interface

Overview: This feature will let users compare two text samples side-by-side and decide which they prefer, in scenarios such as human vs. AI, AI vs. AI, or human vs. human. It will also include a game-like element where users guess which sample is machine-generated. This concept is inspired by past “Turing test” style applications like the Bot or Not poetry game, where judges guess if a poem was written by a human or a computer ￼. In alignment with human-feedback research, the interface will present two alternatives for each prompt to simplify user choice ￼.
	•	MVP Scope: Provide a simple side-by-side comparison of two text passages and allow the user to select a preferred one. After selection, reveal which sample (if any) was AI-generated. Record the user’s choice (and their guess about human vs machine). Initial content can be pre-generated (e.g. a fixed set of human writings and AI outputs) to avoid complexity. The MVP will focus on a clean UI showing two samples with a prompt or context, and basic logging of preferences.
	•	Priority I Features (Must-Haves):
	•	Display two text samples (e.g. Sample A and Sample B) for the same prompt or theme.
	•	Allow the user to choose which sample they prefer (pairwise preference voting).
	•	Allow the user to guess which sample is machine-generated vs human-written before revealing the answer (making it a mini Turing test) ￼.
	•	Support a couple of built-in comparison sets: e.g. one human vs one AI story on a given prompt. (These can be static or fetched from a small dataset.)
	•	After the user submits their choice, immediately reveal which was human and which was AI, and whether their guess was correct – this feedback provides a fun, gamified element.
	•	Log the results of the comparison (preferred sample, the correct answer, the user’s guess) to the database for analysis.
	•	Priority II Features (Future Enhancements):
	•	Dynamic sample generation: Integrate at least two model APIs (e.g. OpenAI GPT-4 and Codex or another model) to generate new text on demand. Users could input a prompt or pick from preset prompts, and the system will fetch outputs from selected models to compare. (For example, the user could generate two AI-written stories from different models, or one AI story to compare with a stored human-written story.)
	•	Multiple model options: Easily extensible to add more models (OpenAI, HuggingFace, etc.) via a modular API integration. In the UI, users can select which models to pit against each other or against human text. The design will account for plugging in new models in the future (e.g. new menu options or a config file of model endpoints).
	•	Custom prompts: Allow advanced users to enter their own prompt or topic to generate comparison samples. This adds engagement and can collect a wider variety of data. The interface would take the prompt, call the APIs to generate two texts, and then present them for comparison.
	•	AI vs AI and Human vs Human modes: In addition to the default human vs AI, include modes to compare two different AI models’ outputs, or even two human-written texts (the latter mostly for baseline or for user curiosity). Human vs human could use two different authors or styles for fun comparisons.
	•	Enhanced feedback for users: After a series of comparisons, show users stats like how often they guessed correctly (i.e., identified the AI) and perhaps how their preferences align with others or with certain model metrics. This could be presented as part of gamification (e.g. “You guessed the AI correctly 70% of the time!”).
	•	Implementation Plan:
	1.	Data & Content Setup: Create a data structure for comparisons. For MVP, this might be a static JSON or a small database table containing pairs of text samples along with metadata (e.g. which one is human, which model produced the other, the prompt, etc.). Ensure each pair has an ID, the text of Sample A and B, and labels for the correct identity (human/model). Optionally include the prompt or context shown to the user.
	2.	Comparison UI: Develop a React component (using Next.js) that displays two text samples side by side (or one above the other on small screens). Each sample should be in a scrollable container if long. Beneath or between them, provide controls: a “Select A” and “Select B” button (or similar UI element like radio buttons or card selection) to record which the user prefers. Also include a toggle or secondary question: “Which one do you think is machine-generated?” with options “A is AI”, “B is AI” (or “Not sure” if applicable). This secondary guess can be captured via a small form or additional buttons.
	3.	Submission Handling: On submission, the app should record the user’s choices. If the user is logged in (see Authentication section), attach their user ID; if not, still record the event (possibly as anonymous or prompt login). Use Supabase or an API route to save the comparison result: which pair ID was shown, which option was preferred, which option the user marked as AI, and perhaps a timestamp. This could be saved in a table like comparisons or responses.
	4.	Reveal Outcome: Immediately after submitting, display a feedback view. This can replace the samples or overlay on them. It should indicate which sample was actually human or model. For example: “Sample A was written by a human, Sample B was by GPT-3.” Also, provide a message if the user’s AI-guess was correct or not (e.g. “You guessed correctly!” or a playful “Oops, that was actually written by a human!”). This step closes the loop for the user and adds a fun learning aspect.
	5.	API Integration (Phase II): Implement server-side logic to generate text from model APIs. For OpenAI, set up an API route (e.g. pages/api/generate.js or Next.js 13 route) that receives a prompt and model choice, calls the OpenAI completion API, and returns the generated text. Use OpenAI’s SDK or REST API – given any text prompt, the API will return a completion ￼. For example, call OpenAI’s GPT-4 for story generation and perhaps another model (Codex, GPT-3.5, or a HuggingFace model via their API) for a second story. Ensure API keys are stored securely (in environment variables) and not exposed to the client.
	6.	Dynamic Generation UI: Extend the comparison UI to allow the user to trigger generation. This could be a form at the top of the comparison page: the user selects a prompt (or enters one) and chooses two sources (e.g. “Human vs GPT-4” or “GPT-4 vs GPT-3”). On form submission, call the backend to fetch the necessary texts. For a human sample, you might randomly pick one from a curated set of human-written stories based on the prompt or topic; for model samples, call the model APIs. Once the two texts are obtained, display them in the comparison interface for the user to evaluate.
	7.	Extensibility for Multiple Models: Architect the generation logic to be modular. For instance, have a function or service class for each model API (OpenAI, etc.) implementing a common interface (e.g. generateText(prompt, modelName) which internally routes to the correct API). This way new models can be added by plugging in new handlers. Also, maintain a list of available model options (for UI dropdowns) and their parameters (max tokens, etc.). Document clearly how to add new models.
	8.	Edge Cases & Quality: Put safeguards for very short or very long texts (maybe enforce a moderate length for better comparison). Also ensure the content generated is appropriate (potentially use OpenAI’s content filters if available, or have a disclaimer since this is a research platform). If an API call fails or times out, handle gracefully (e.g. show an error message or retry option).
	9.	Testing: Before launch, test the comparison flow with various scenarios: known human vs AI pairs (ensure the reveal is correct), AI vs AI generation (both outputs show up), and that all data (preferences and guesses) is correctly stored. Also test UI responsiveness (side-by-side on desktop, stacked on mobile) and refine for readability (e.g. distinct font or background for each sample container).
	•	Dependencies / APIs:
	•	OpenAI API (Codex/GPT): Used for generating AI text completions. The OpenAI API provides a versatile “text in, text out” endpoint for prompts ￼. We’ll use it (or similar services) to dynamically produce model outputs. See OpenAI’s API reference for endpoints like completions or chat.completions (for GPT-4). Documentation: OpenAI API Reference ￼.
	•	Hugging Face Inference API (optional): For additional models, Hugging Face’s hosted inference endpoints can be integrated. This can enable using open-source models for variety. Documentation: Hugging Face Inference Endpoint docs.
	•	Next.js API Routes or Edge Functions: We will leverage Next.js serverless functions to call the external APIs securely (keeping keys on the server). No additional library needed beyond what the model APIs require (e.g. openai npm package for OpenAI).
	•	Supabase (Database): The platform will use Supabase/Postgres to store comparison results. A table (e.g. comparisons) will record: user_id, sampleA_id, sampleB_id (or the text content if needed), preferred_choice, guessed_human (which the user flagged as human), and timestamp. Supabase JS library will be used to insert these records from the frontend or via RPC.
	•	Frameworks/Libs: Basic React/Next components for the UI. Possibly a UI library (like Chakra or Material) for layout and styling of cards/buttons, though custom styling is fine. We may also use a state management solution (React Context or Zustand) if needed to handle passing generated texts to the display component.
	•	Technical Considerations:
	•	Separation of Concern: Keep the generation logic on the server side. The client should never directly call the OpenAI API (to protect the API key). Instead, the client triggers a Next.js API route which handles the call and returns the text. This follows best practice for using third-party APIs from Next.js, ensuring secrets are safe and usage is metered on the backend.
	•	Response Time & UX: Calling AI APIs can introduce latency (e.g. a couple of seconds to generate a story). To keep the UI responsive, implement a loading state – e.g. a spinner or placeholder text – while generation is in progress. Optionally, use streaming for OpenAI (they support streaming tokens) to show text as it’s generated, but this complicates the client side. An easier approach is to fetch the result and then display it all at once with a loading animation in the meantime.
	•	Prompt & Length Management: Ensure the prompts sent to models produce comparable length outputs. We might fix a token limit or ask for a certain word length in the prompt. Consistent length avoids one sample being much shorter (which could bias user preference).
	•	Extensibility: When adding new models or data sources, avoid hard-coding assumptions. For example, define a config for model endpoints and names. The UI can populate model options from this config. Also, design the data schema to accommodate new model identifiers (perhaps a field for model name in the comparisons table, to know which model was used for an AI sample).
	•	Security: Although the text content is generally not sensitive, be mindful that users could input any prompt. Implement basic sanitization if displaying user-entered prompts back on the page. Also handle potential API abuse (maybe rate-limit generation requests per user to prevent spam or excessive costs).
	•	Comparison Count & Analytics: Over time, as users perform comparisons, the database will accumulate results. Plan how to analyze or export this data (perhaps via the Parquet download feature). From a design perspective, ensure each comparison record links to what prompt and model was used for future analysis of human-vs-AI preferences.
	•	UI/UX: Make the interface intuitive – label the samples generically as “Text A” and “Text B” to not bias the user. Only after the user makes a choice do we label them as human/model. Consider a small tool-tip or info icon explaining that one of the texts might be AI-generated, so the user knows the context of the game. Also, ensure it’s easy to start a new comparison after finishing one – e.g. a “Next Comparison” button or automatic loading of a new pair – to keep users engaged in a session.

2. User Interaction & Engagement Features

Overview: This set of features aims to make the platform more engaging and interactive, encouraging users to spend time reading and providing feedback. We will implement the ability for users to highlight the best part of a story, justify their preferences with a written reason, and introduce creative UI/UX elements (like progressive reveal of content and gamification) to sustain user interest. The rationale is that modern users often have short attention spans, so a gamified, interactive reading experience will help maintain engagement ￼.
	•	MVP Scope: Allow users to annotate their choice by highlighting their favorite sentence (or passage) in the story they preferred, and optionally entering a brief text explaining why they chose that sample. These interactions (highlights and reasons) will be captured along with the comparison results. The MVP focuses on basic functionality: text selection and a text input for explanation. It will also include a simple reveal mechanic (e.g. perhaps showing one story at a time or hiding the identities until after voting) to keep the experience focused. Gamification at MVP can be minimal, such as a score counter for correct guesses or a congratulatory message on completion, laying the groundwork for more elaborate features later.
	•	Priority I Features (Must-Haves):
	•	Highlight Best Sentence: Users can select a sentence (or short span of text) from a story and mark it as the “best part.” This could be implemented by letting them highlight text on the page. Upon selection, the system could show a “Highlight” button or automatically save the selection as the highlight. Visually, the highlighted sentence might be emphasized (e.g. yellow background) for confirmation. Storing the highlighted excerpt (or its indices) is important so we know what content the user loved most. This feature turns passive reading into an active interaction, similar to how Medium allows readers to mark notable quotes ￼.
	•	Reason for Preference: After choosing which sample they prefer in the comparison, prompt the user with a short text box: “Why did you prefer this story?” The user can type a sentence or two explaining their reasoning (e.g. “It had more vivid imagery” or “The ending was surprising”). This qualitative feedback is valuable to understand user choices. The interface should make this optional but encouraged (perhaps with a placeholder like “Optional: tell us why…”). Save this free-form text to the database linked to the comparison result.
	•	Basic Gamification Elements: Integrate small rewards or interactive prompts to keep users engaged. For example, show a running score or streak of how many AI-vs-human guesses the user has gotten correct so far (“🏅 3 out of 5 correct so far!”). Additionally, incorporate a progress indicator if there’s a set number of comparisons or just to show how many they have completed. These elements tap into users’ competitive instincts and can motivate continued participation ￼.
	•	Progressive Content Reveal: To combat short attention spans, avoid overwhelming the user with too much text at once. A simple implementation: reveal the two samples in stages. For instance, show the prompt and the first sample initially; once the user finishes reading the first sample (maybe they click “Next”), reveal the second sample. This ensures the reader focuses on one at a time. Alternatively, if both are shown together, consider collapsing long text with a “Read more” expansion. The key is to present content in digestible chunks so that the user stays engaged throughout.
	•	User Guidance and Tooltips: Provide subtle guidance for new users. For example, a one-time tooltip that says “Click and drag to highlight your favorite sentence!” or a small tutorial overlay for the first use can improve feature discoverability. This ensures users know they can highlight and explain preferences, rather than skipping those features.
	•	Priority II Features (Nice-to-Haves):
	•	Enhanced Gamification & Rewards: Build on the basic scoring with richer game mechanics. This could include badges or achievements (e.g. “Literary Critic – provided 10 reasons”, “AI Detector – 5 correct AI guesses in a row”) ￼ ￼. Another idea is a leaderboard or community stats, showing top participants or overall percentages of correct guesses (for a competitive or community feel). We can also introduce levels or points for each action: e.g. +10 points for each comparison done, +5 for providing a rationale, +1 for each highlight. Points accumulate and could unlock fun titles or simply be displayed to the user as their score.
	•	Narrative or Storytelling Mode: Make the reading experience feel like a game or story. For example, a mode where the user is presented with a series of comparisons as “challenges” or a quest (gamified narrative). After finishing a batch, show a summary of their “performance.” This framing can make the evaluation tasks feel like an interactive story in itself.
	•	Visual Feedback for Highlights: Improve the highlighting UX by maybe allowing multiple highlights or showing a heat-map of commonly highlighted sentences (if collective data is used). For the individual user, after they highlight, we could display a small tooltip “Marked as best line!” or allow them to remove/change the highlight before finalizing.
	•	Social Sharing or Discussion: If appropriate, let users share interesting findings (e.g. a particularly good AI story snippet) or discuss why they chose one over another. This could be via a dedicated discussion board or simply a share button that posts a quote (the highlight) and a link to the site, leveraging their highlights. (This idea draws inspiration from Medium’s highlight-sharing feature that encourages discussion around content ￼.)
	•	Adaptive Content Length: Recognize user’s attention patterns and adapt: e.g. if a user tends to drop off on longer texts, perhaps serve them shorter comparison texts or chunk a story into segments with a continue button (progressive reveal in a dynamic sense). This is an advanced idea requiring tracking reading behavior (like scroll depth or time spent) and could be a future improvement for personalization.
	•	UI Polish and Animations: Add pleasant animations to the UI interactions. For example, when the user selects a preference, animate the selection (card pulses or is highlighted), or when revealing which was AI, maybe flip the card or show a small robot icon on the AI one. Highlights could be accompanied by a slight underline animation. These subtle touches make the experience more enjoyable. Using libraries like Framer Motion for React can help implement these without impacting logic.
	•	Implementation Plan:
	1.	Highlighting Mechanism: Implement a way for users to highlight text on the page. One approach: break each story’s text into individual sentences or spans in the HTML (possibly by splitting on period/exclamation/question for rough sentence segmentation). Wrap each sentence in a <span> with an identifier. Then, add an onClick or onMouseUp handler on the text container that checks for a text selection (using window.getSelection() API). When the user selects some text and releases, if the selection length is above a minimal threshold, capture that string (or the start-end indices). Save this selection to state and visually apply a highlight style (e.g. a CSS class that changes background color) to the selected span(s). Alternatively, for simplicity, limit highlights to a single sentence: a user could click a “highlight” button next to each sentence. But a free-form text selection is more flexible and closer to typical UX.
	2.	Recording Highlights: When the user finalizes the comparison (submits their vote), also submit the highlight data. The highlight data could be stored as the exact text snippet they highlighted (easiest) or as indices referencing the story. Storing the text excerpt (maybe up to a certain length) is straightforward and avoids indexing issues. Create a database field in the comparisons/annotations table for highlight_text. If we expect multiple highlights, we could store an array or separate table, but since we constrain to “best part” we can assume one highlight per comparison.
	3.	Reason Input: After the user picks a preferred story, reveal a small text input area. (This can be done conditionally: e.g. once a preference radio is selected, the reason textbox appears below). Keep it simple with a 1-2 sentence suggestion length. In the form submission, include this reason text. In the database, store it in a reason or feedback column associated with the comparison entry. On the UI, validate or limit length (e.g. max 300 characters) to keep it concise. Possibly make this field optional – but in analysis we’d like as many as possible, so we might use a prompt like “(Optional) Why did you prefer that story?” to encourage input without forcing it.
	4.	Gamification Elements: Introduce a global state for user’s score or streak. For MVP, this could be stored client-side (in React state or localStorage) just to display feedback like “X correct guesses”. Each time a comparison result is revealed, update this state: if the user guessed the AI correctly, increment their correct count. Show this count in a corner of the interface or in a navbar (“Score: 5”). If the user is logged in, consider persisting this score server-side (or compute from their history). But initially, a client-only score is fine for engagement (since users could refresh and lose it, but that’s acceptable early on). Also provide immediate feedback after each guess: maybe an emoji or small animation (e.g. confetti effect if correct – libraries like canvas-confetti can do this easily). This reward feedback ties into proven gamification techniques of positive reinforcement ￼.
	5.	Progressive Reveal UI: Instead of showing both entire stories at once, implement a step-by-step reveal. For example, show Story A with a “Next Story ➡️” button. When clicked, show Story B. Only after reading Story B, show the voting buttons. This ensures the user doesn’t just skim both at once or scroll too fast. To do this in Next.js, the component state can track which stage the user is in (story1 shown, or both shown). Initially render only the first text container and a call-to-action button. On click, update state to render the second text container and the voting options. Alternatively, use a modal or accordion approach (Story B is initially collapsed). This also adds a bit of suspense to the experience, which can make it feel more interactive.
	6.	Tutorial/Tooltips: Implement a one-time tutorial overlay. A simple approach: when a new user (first visit or first login) lands on the comparison page, check a flag (maybe in localStorage or in user profile) if tutorial was shown. If not, darken the background and show a few pointers: e.g. a highlight around the text area saying “Read the stories”, an arrow pointing to the selection buttons “Choose your favorite”, and an arrow pointing to the highlight feature “Select text to mark the best part!”. This can be done with a library like react-joyride or manually with styled components and a bit of state. Allow skipping the tutorial. Once done or skipped, set the flag so it doesn’t show again.
	7.	Backend Storage Adjustments: Extend the database as needed: ensure the table capturing user comparisons has fields for highlight (text) and reason (text). If these are large text fields, make sure the DB column types (e.g. text in Postgres) can accommodate them. Also store something like correct_guess (boolean) if the user’s guess was right, so we can calculate scores later. With Supabase, we may create a table user_feedback with columns: id, user_id, comparison_id (if linking to a comparisons table of predefined pairs), preferred_sample (A/B), guess (A/B), highlight_text, reason_text, correct_guess, timestamp. This structured approach helps in analyzing the data later (e.g. see common words in reasons for AI vs human preferences, etc.).
	8.	Testing & UX Tuning: Thoroughly test highlight selection on different devices and browsers (text selection can be tricky on mobile vs desktop). Ensure that the highlight action doesn’t interfere with the normal copying or scrolling. Also verify that submitting without a reason still works (i.e. reason optional). For the progressive reveal, test that users cannot bypass (maybe disable the vote buttons until both stories are revealed). Gather some initial user feedback if possible to see if the interactions are clear. Usability is key: if users don’t notice they can highlight or think the reason is required and get annoyed, adjust the UI cues accordingly (e.g. add a subtle prompt “Highlight a sentence you loved!” near the end of each story).
	•	Dependencies / Libraries:
	•	Text Highlighting Libraries: Although not strictly required, libraries like highlight.js are more for syntax/code highlighting, not user selection. For our use, native JS might suffice. If needed, rangy or React Text Highlighter could be used to simplify selection handling. Another approach is using a content-editable field or a rich-text editor library (like Draft.js or Slate) to capture highlights, but that may be overkill. We will likely implement manual selection logic for full control.
	•	React Tooltip / Joyride: For onboarding tooltips or guided tours, we can use react-joyride (a popular tour library) or intro.js to create the interactive tutorial steps. These help highlight parts of the UI and display messages for first-time users.
	•	Confetti or Feedback Animations: If adding fun effects, canvas-confetti (tiny JS library) can shoot confetti on correct guess. Framer Motion can animate component transitions (like fading in Story B, or animating the highlight). These dependencies are optional but can enhance UX.
	•	Supabase/Database: Ensure the database is updated to store the new fields (highlight, reason, etc.). Supabase JS will be used to send these along with the comparison result. We might use a single API call to our backend to handle saving everything from a round (preference, guess, reason, highlight in one go) to avoid multiple calls. Alternatively, use Supabase directly from client if RLS allows (the user can insert into a table if properly authorized).
	•	Analytics (optional): If we want to track engagement (like how many highlights are made, or where users drop off), integrating an analytics solution (e.g. Google Analytics or a custom event logger) could help. This is not a core dependency but useful for iterative improvement on engagement features.
	•	Technical Considerations:
	•	Data Linking: Each user’s highlight and reasoning should be linked to the specific story sample it refers to. If we have pre-defined story pairs, we know which text the highlight came from (A or B). But for clarity, we might want to store not just the text of the highlight but also which sample (A or B) it came from, or an identifier of the story. This helps later if we want to extract, say, the most highlighted sentences of AI-written texts vs human texts.
	•	Highlight Storage vs Display: There is a design choice whether to display the user’s highlight to them after submission (to confirm what they selected). Likely yes, we keep it highlighted on screen. But if the user navigates away and back, do we retrieve and re-render their past highlights? For now, since the platform is more for data collection and user enjoyment in-the-moment, we might not need to persist and show highlights across sessions. It could be one-time per comparison. In the future, if users can revisit comparisons or have a profile of their past activity, we could show their highlights there.
	•	Privacy and Courtesy: If we later show aggregated popular highlights (like “80% of users highlighted this sentence”), we should anonymize and perhaps only show if enough users have participated to avoid exposing a single user’s opinion. This is more of a future analytics idea, but it’s good to note early on.
	•	Balancing Engagement vs Fatigue: While adding interactive elements, be mindful not to exhaust the user. For example, if every comparison asks for a written reason, some users might get tired. We can monitor drop-off rates. Possibly make the reason optional and gracefully handle when it’s left blank. Similarly, highlighting is optional – the UI should not force it, but it should encourage it. Use gamification to reward these actions (like small bonus points for providing a reason) to gently push users to engage without making it mandatory.
	•	Responsive Design: Ensure that the UI elements for highlight and input don’t clutter small screens. On mobile, selecting text can also trigger copy/paste tooltips by the OS – we might need to test and possibly advise users (or use a different approach like tap-to-select a sentence). Also, the reasoning input on mobile should be a proper textarea that expands, and not hidden behind the keyboard. All interactive buttons should be adequately large for touch input.
	•	Content Volume: If stories are long, the progressive reveal strategy can help, but we should also consider limiting the length to what a user can reasonably read in a short session (maybe a few paragraphs each). The literature interest of users will vary, but shorter content might lead to more engagement. We could even experiment with truncating stories and seeing if users want to “read full story” (a form of progressive disclosure). Gathering such metrics can inform content length decisions.
	•	Future Personalization: The collected metadata (age, interest, etc. from onboarding) could potentially inform the engagement features. For instance, younger users might prefer more gamified interfaces. Although not in MVP, we might consider toggling certain UI aspects based on user profile (this ties the engagement with user metadata, see section 5). For example, if a user indicates high interest in literature, perhaps they get longer stories or an option to read the full research paper. While this is speculative, it’s good to design the system flexibly to accommodate such personalization down the line.

3. Paper Viewing Interface

Overview: The platform is likely accompanied by a research paper (e.g. describing the human vs AI comparison study). We need to provide an interface for users to read this paper directly on the site. This feature will add a “Paper” or “Research” tab on the homepage that allows viewing either a PDF or an HTML version of the paper. The emphasis is on a clean, user-friendly document viewer embedded in the site, so that users (especially researchers or interested readers) can easily access the full publication without leaving the platform.
	•	MVP Scope: Offer a straightforward way to read the paper in PDF form. The simplest MVP is to embed the PDF using a standard PDF viewer component or browser plugin. For example, an <iframe> or HTML <object> can load the PDF file. The MVP would have a tab or link (e.g. “Read the Paper”) that opens the viewer with basic functionality: page scrolling, zoom in/out, and perhaps a download button. The PDF should be pre-compiled and stored (likely in the public assets or a CDN) so it loads quickly. At MVP, we assume the PDF exists and is static; no need for live LaTeX rendering. The focus is on ensuring the document is readable on both desktop and mobile (the viewer should be responsive or at least scrollable on smaller screens).
	•	Priority I Features (Must-Haves):
	•	Embedded PDF Viewer: Integrate a reliable PDF viewing component into a dedicated page or modal. A good option is to use a React PDF library to handle rendering. For instance, react-pdf allows displaying PDFs easily in a React app ￼. It uses Mozilla’s PDF.js under the hood, providing out-of-the-box rendering of PDF pages on a canvas. We should utilize such a library to have controls like pagination (next/prev page), zoom controls, and maybe search within the PDF. At minimum, users should be able to scroll through the pages and read all content clearly.
	•	Accessible via Tab on Homepage: Add a prominent navigation element (e.g. a tab in the top menu or a section on the homepage) labeled “📄 Research Paper” or similar. Clicking it should load the Paper viewer interface. This could be implemented as a Next.js page (e.g. /paper) or as a modal overlay. A separate page is simpler and allows direct linking (e.g. we can share the URL /paper to others). Ensure that navigating to this page doesn’t log out the user or disrupt ongoing comparisons (if needed, open in new tab or use Next’s  for smooth transitions).
	•	Smooth Viewing Experience: Use a “clean” viewer UI, meaning minimal distractions. Ideally, the viewer should fit within the site’s frame nicely. We might hide overly complex toolbar options. If using react-pdf or a similar library, we can build a custom minimal toolbar (just page nav and zoom). Alternatively, another library react-pdf-viewer provides a full-featured toolbar and layout out of the box. The design goal is that users feel like they are reading a document, not just seeing an image. Text should be selectable (for copy-paste) if possible, and zooming should not degrade readability.
	•	Basic Controls: Provide basic controls like Zoom In/Out (to make text larger or fit more on screen) and Page Navigation (jump to page, next/prev). Many PDF viewer libraries include these. For example, react-pdf allows rendering one page at a time; we can add buttons to increment page number. We should also show the current page number and total pages (e.g. “Page 3 of 10”). This helps orientation within the paper.
	•	Download Option: Even though the paper can be read online, some users might want to download it. So include a Download button or link that fetches the PDF file directly (opening it in the browser’s PDF viewer or saving to disk). This can simply be an anchor link to the PDF file URL (if the PDF is hosted publicly). If we want to restrict it to logged-in users, the link can be gated similar to the Parquet file (though papers are often public, so it might not be sensitive).
	•	Priority II Features (Enhancements):
	•	LaTeX to HTML Rendering: Provide an alternative HTML view of the paper. This could be useful if we want a responsive, mobile-friendly version or to allow in-browser text search and hyperlink navigation. Tools like LaTeX.js or converting the paper to Markdown/HTML could be explored. For example, if the paper is on arXiv, we could use the arXiv API or third-party like ar5iv (which converts arXiv TeX to HTML) to embed an HTML version. An HTML version would allow copying text more easily and could be styled to match the site’s theme. However, this is a non-trivial task if the paper has formulas, figures, etc. (MathJax would be needed for equations). This is a stretch goal for better UX.
	•	Search and Navigate: Enhance the PDF viewer with a text search function (to find keywords in the paper) and a sidebar with outline/sections if available. Some PDF viewers have a sidebar for table of contents or thumbnails. For a research paper (~8-10 pages perhaps), thumbnails or a TOC might not be critical, but it’s a nice feature. Search is more valuable if a user wants to quickly find a section (especially if referencing the paper for details while using the platform).
	•	Annotations or Highlights: Allow users to highlight or comment on the paper itself. This could be useful for community engagement (though it veers towards a specialized PDF annotation tool). Perhaps simpler: allow users to select text in the PDF and copy it (which PDF.js supports by default if text layer is enabled). Full annotation might be overkill, but a read-only highlight of key contributions (pre-highlighted by the authors) could be interesting. For example, highlight the part of the paper describing the dataset or the results. This could draw readers’ attention to crucial info.
	•	Mobile Optimization: On smaller devices, PDFs can be hard to read. As an enhancement, implement a responsive mode where maybe the PDF page is shown one at a time and can be swiped or tapped through (like an e-book reader). Possibly detect mobile and use a different viewer configuration (larger text or only portrait orientation). If an HTML version is available, mobile users might prefer that due to reflowable text.
	•	Loading Performance: If the PDF is large or the viewer loading is slow, optimize by using techniques like PDF.js’s progressive loading. Possibly provide a loading spinner when the user opens the paper tab, since PDF rendering might take a second. For enhancement, prefetch the PDF in the background when the user is on the homepage, so if they click “Paper”, it opens instantly. Next.js can prefetch routes by default if using . We can also leverage browser caching by setting appropriate headers on the PDF file.
	•	Multiple formats: Offer alternate formats for download, such as BibTeX for citation, or a link to the paper’s DOI/arXiv page. Not exactly “viewing interface” but part of the paper’s presence: e.g. “Download PDF | Download Citation”. This is minor but useful for academic users.
	•	Implementation Plan:
	1.	Acquire/Host the Paper: Determine how the paper will be stored and served. If it’s a PDF, we can place it in the Next.js public folder (e.g. public/paper.pdf) so it’s accessible at https://ourdomain.com/paper.pdf. Alternatively, store it in cloud storage and fetch it. But given likely small size (a few MB at most), embedding in the app is fine. Ensure we have the final compiled PDF (from LaTeX or source). If updates to the paper occur, remember to replace this file.
	2.	Set Up Paper Page/Route: Create a new Next.js page (if using the Pages router, pages/paper.jsx; if using App router, perhaps a route segment app/paper/page.jsx). This page will contain the Paper viewer component. Protect it behind navigation such that it’s easy to find. Possibly add a link in the main navigation bar (conditionally, we might want it always visible). If using a tabbed interface on homepage (less likely with Next pages, but we could simulate with client-side routes), ensure that clicking the tab doesn’t reload the whole app unnecessarily – but a full page load is okay for PDF.
	3.	Integrate PDF Viewer Library: Install react-pdf (which consists of components <Document> and <Page>). Also install required peer dependencies (it needs PDF.js worker; the library docs explain usage). For example, we’ll use:

import { Document, Page } from 'react-pdf';

In our component state, keep track of numPages (total pages) and pageNumber (current page). We can load the PDF by providing the file URL to <Document file="/paper.pdf" onLoadSuccess={onDocumentLoad}>. The onDocumentLoad callback will give us numPages. Then we render <Page pageNumber={pageNumber} /> inside the Document. This will draw that page as an SVG/canvas. We then add controls: a button to decrement pageNumber (if >1) and increment (if < numPages), and maybe a dropdown or input to jump to a specific page. Also include a zoom control: react-pdf allows setting a scale on Page (or we could simply use CSS transform).
Alternatively, consider using @react-pdf-viewer/core which provides a pre-built viewer toolbar. But it might be heavier. For full control and minimalism, using react-pdf with custom controls is acceptable.

	4.	Address Next.js SSR Issues: By default, PDF.js (used by react-pdf) may not work during Server-Side Rendering. Next.js might try to SSR the component and run into issues (like requiring Canvas or failing to load the worker). The known solution is to dynamically import the PDF viewer component with ssr: false so it only renders on the client ￼. For example, we create a component PDFViewer.jsx that contains the <Document> and  logic. In the Paper page, we do:

const PDFViewer = dynamic(() => import('../components/PDFViewer'), { ssr: false });

This ensures the PDF rendering happens only in the browser. We also need to configure the PDF.js worker loader. The react-pdf docs suggest setting //pdfjs.GlobalWorkerOptions.workerSrc. But an easier method is to use their <Document> worker prop or include <Worker> from react-pdf. We point it to the pdf.worker.js from PDF.js (which we can host or use from CDN). This technical setup is important for the viewer to function properly in Next.js (especially Next 13+), as noted in community solutions ￼.

	5.	UI Layout and Styling: Design the interface such that the viewer occupies most of the space. Maybe have a header with the paper title or just rely on the PDF’s content. If we want the site’s header to remain, we can embed the viewer below it. On large screens, perhaps center the PDF or give it a container with some margin for better readability. On small screens, make it full-width. The controls (zoom, page nav) can be a fixed bar either at top or bottom of the viewer. Keep the styling consistent with the site (e.g. use the same font for any UI text, same color scheme for buttons). Possibly include a “Close” or “Back” button to return to main app if the paper is on a separate page (or instruct users they can just use menu to navigate away).
	6.	Testing PDF Functions: Test that the PDF loads correctly in various browsers. Particularly, ensure the worker is loaded (the PDF will not display if the worker fails). Check console for any errors about PDF.js. Also test the zoom – does text stay clear on zoom (it should, since PDF is vector text). Check memory usage for large zoom or many pages; typically not an issue for one paper but be mindful on mobile browsers. Also verify if text selection works (react-pdf by default enables a text layer that should allow copy-paste; if not, we might need to enable the textLayer prop).
	7.	Optimization: If initial load is slow (blank for a couple seconds), consider adding a spinner and/or splitting the PDF into chunks. However, given the paper’s size is moderate, loading entirely then rendering is fine. We might leverage Next.js static file serving which should be quite fast. If needed, instruct Next to prefetch the page. If using <Link href="/paper">, Next may prefetch that route’s code. But since we turned off SSR and the PDF file is static, it mostly comes down to network speed for the PDF. Perhaps set HTTP headers for caching the PDF long-term.
	8.	Alternate Format (Phase II): If we decide to implement an HTML version, the plan would involve either manually creating an HTML (for example, converting the LaTeX to HTML once and serving it) or embedding an external viewer. For instance, if the paper is on arXiv, using the ar5iv HTML might be possible by embedding an iframe to ar5iv’s content. But that requires internet and might break styling. Alternatively, use a library like Pandoc offline to convert LaTeX to HTML and then include that in the app. This is a complex task due to equations and figures, so it’s a future goal. If done, we’d add a toggle on the paper page “View as HTML” vs “View PDF”. The HTML view would need custom CSS but could allow things like mobile-responsive text reflow and hyperlink navigation (e.g. clicking references).
	9.	Enhancements Implementation: Add a search bar for the PDF by leveraging PDF.js text capabilities. This might require accessing the text content of each page. Some libraries provide a search plugin. If using react-pdf-viewer, they have a search plugin ready. If not, we could implement basic search by scanning text strings of pages (react-pdf gives access to page text items). Given time, this would be a later addition. Also, if adding a sidebar with outline: if the PDF has a table of contents metadata, PDF.js can access it. But more practically, we could hardcode section links if needed. Evaluate cost/benefit of these enhancements as the platform grows.

	•	Dependencies / Libraries:
	•	react-pdf (Mozilla PDF.js) – primary library for PDF rendering in React ￼. We will use this for core functionality. Link: React-PDF on npm.
	•	@react-pdf-viewer (optional) – an advanced PDF viewer React component with plugins for toolbar, search, etc. If we prefer not to build our own controls, this could be integrated. It’s a bit heavier but offers ready-made UI. Link: react-pdf-viewer documentation.
	•	Next.js dynamic import – to handle the no-SSR requirement for PDF viewer. (Built-in feature of Next.js, no extra dependency, but a crucial technique).
	•	PDF.js Worker file: Either use the one bundled with react-pdf or host the PDF.js worker. The react-pdf README usually instructs to do: import { pdfjs } from 'react-pdf'; pdfjs.GlobalWorkerOptions.workerSrc = "url-to-pdf.worker.js";. We may need to copy the worker file to our public folder or use a CDN link provided by pdfjs-dist. This is necessary for parsing the PDF in a separate thread.
	•	Testing on Browsers: Ensure to test on Chrome, Firefox, Safari as PDF rendering can sometimes differ (especially on Safari iOS, which might insist on its own PDF viewer – if so, possibly just fallback to a direct link for iOS). Not exactly a dependency, but a testing step.
	•	Technical Considerations:
	•	No SSR for PDF Components: As noted, must disable server-side rendering for components using Window or Canvas (PDF rendering uses Canvas). This means the Paper page might not be SEO-indexable in terms of content (since the PDF text isn’t pre-rendered). That’s fine because it’s mostly for user reading, not for search engines. Just be aware that react-pdf should be wrapped to only run on client. We already plan dynamic import for that ￼.
	•	File Size and Bandwidth: A PDF could be a few MB. This is generally fine; however, for slower connections it might be noticeable. We could compress the PDF if not already (most PDFs are reasonably compressed). Alternatively, provide an outline or summary in text for those who can’t or won’t load the PDF (maybe on the paper page, include the abstract or a link to a text summary). This could be a simple addition: e.g. show the paper’s abstract and citation info above the viewer, so users know what they’re about to read and in case the PDF fails they got the gist.
	•	Security: Serving a PDF is low risk. But ensure the PDF file is not accessible to unauthorized users if it’s not meant to be public. In our case, likely the paper is public or at least accessible to the intended audience. If it needed to be restricted, we’d have to implement an auth check before serving (which complicates things, since static file serving in Next won’t do auth). We might then serve it via an API route that checks session and then streams the PDF. However, since the prompt doesn’t indicate the paper is confidential, we assume it can be public or at least available to all logged-in users.
	•	Maintaining Context: When users switch to the paper tab, consider if we want them to easily return to the comparison interface. If the site uses a client-side router, the state (like ongoing comparisons) might be preserved. If it’s a full page load, maybe provide a clear navigation back. For example, a “Back to Home” or menu so the user doesn’t feel stuck on the paper page. Also, if a user opens the paper in the middle of doing comparisons, we might warn unsaved progress – though in our case, each comparison submission is immediate, so not much to lose by navigation.
	•	PDF Versioning: If the paper gets updated (new revision), ensure to update the file. Might consider adding a version number or date on the page to indicate freshness. If using caching, cache invalidation when updating the PDF is necessary (e.g. change the filename or use query param).
	•	Alternate Approach: We briefly consider if using an external service like Google Drive Viewer or embedding an HTML  pointing to the PDF’s URL would suffice. That’s very low-effort (just ). Modern browsers will display the PDF with built-in controls. However, this approach depends on browser PDF support and yields inconsistent UI (and possibly not as integrated feel). It also might not work well on mobile (some mobile browsers download instead of view). Using our own viewer ensures a consistent experience and theme, so it’s preferable despite a bit more implementation work.
	•	Future Interactive Papers: A fanciful idea is if we allow the research paper to be interactive (embedded charts or code). But that’s beyond scope. Still, we might keep the paper page ready for additional content like supplementary materials or a link to the dataset (which relates to the next section). This way, the “Paper” section becomes a mini resource hub for scholarly info about the project.

4. Parquet File Download

Overview: We want to host a dataset or relevant data in Parquet format for users to download. Parquet is a columnar storage file format commonly used for large datasets. The platform should provide a reliable download link for this file, while ensuring only authorized users (e.g. logged-in users) can access it. We also need to choose a cloud storage solution to host the file that is cost-effective and scalable. Examples include AWS S3, Google Cloud Storage, or newer services like Cloudflare R2 or Backblaze B2. The solution should balance cost (especially egress bandwidth costs) and ease of integration with our tech stack. Authentication gating is crucial so the data isn’t freely accessible to the whole internet (assuming the dataset is intended for registered users only).
	•	MVP Scope: Provide a download button or link on the site that delivers the Parquet file to the user’s computer, only if they are logged in. For MVP, a simple approach is to store the file in a private bucket and generate a short-lived pre-signed URL when the user clicks download. This URL will allow that user to download the file directly from the storage (e.g. S3) for a limited time (say 1 minute), preventing hotlinking or unauthorized access. The MVP requires that the user be authenticated in our app; if not, clicking the download prompt should either show a “Please log in to download” message or redirect to login. The actual hosting could initially be on AWS S3, given its reliability, and we’ll handle the auth gating via our backend.
	•	Priority I Features (Must-Haves):
	•	Secure File Hosting: Choose a storage provider for the Parquet file. AWS S3 is a common choice, with stable performance. We create an S3 bucket (or use an existing one) to store dataset.parquet. The bucket should be private (no public read access). Our app (server side) will have credentials to access it. Alternatively, if using Supabase, Supabase’s Storage could be used (it wraps S3 under the hood) with RLS rules – but direct S3 gives more control.
	•	Pre-signed URL Generation: Implement an API endpoint (e.g. Next.js API route /api/get-dataset-link) that checks the user’s auth status (perhaps by verifying a Supabase JWT or session cookie) and, if valid, generates a pre-signed URL for the Parquet file. In AWS SDK for Node.js, this is done with getSignedUrl function (from S3 client), specifying the bucket, key, and an expiry. This signed URL will allow a one-time download without further auth. It will be returned to the frontend. The URL should expire relatively quickly (e.g. 60 seconds) to prevent sharing.
	•	Download Button (UI): On the frontend, possibly on a “Data” or “Download” section (maybe the homepage or profile has a section for data resources), provide a Download Dataset button. When clicked, it calls the above API (using fetch). On success, it receives the URL and then we can programmatically trigger the file download. For example, set window.location = thatSignedUrl which causes the browser to start downloading the file. Alternatively, we can present a link and prompt user to click it. The UI should also display the file name and size (if known) so the user has context (e.g. “Download dataset.parquet (50 MB)”). If the API call indicates not logged in (e.g. returns 401), the UI could redirect the user to login or show a modal.
	•	Authentication Enforcement: Only logged-in users should get the link. We will rely on Supabase authentication for this. There are a couple of ways:
	•	If we use Supabase Edge Functions or RPC: We could create a function that checks auth.uid() then returns a signed URL (Supabase Storage has a signing capability but if we use S3 outside Supabase, we do it in our Node API).
	•	If using a Next API route directly: We need to verify the user’s session. This can be done by requiring the client to pass a session token (the Supabase JWT) in the request, or if using Next middleware, by reading a cookie that Supabase auth sets. Supabase’s JS SDK can provide the logged-in user’s access token. Our API route can validate it (Supabase provides an endpoint or we decode the JWT’s signature with Supabase public key). Simpler: since the front-end calls this only when user is logged in (we can gate the UI), a minimal check could be to trust that and generate the URL. But for security in depth, verifying the token is better.
	•	For MVP, an easier path is to check via Supabase’s client: the user’s session is stored client-side, so we know if logged in. Only show the download button if user !== null. Then call the API route with an included auth header (maybe the supabase token). The API route decodes token or perhaps uses Supabase admin client to get user by token.
In summary, ensure that without auth, one cannot get the signed URL.
	•	Cloud Provider Choice: In terms of cost-effectiveness, consider Cloudflare R2 as an alternative to S3. Cloudflare R2 is S3-compatible and has no egress fees ￼, which could significantly cut costs if the dataset is large or downloaded often. For MVP, we might stick to S3 (quick to implement if we have AWS creds). But we will mention R2 as a recommended provider for production due to zero egress cost (AWS S3 charges for data out, which can add up) ￼. GCP Cloud Storage is another option with similar pricing to S3. Wasabi or Backblaze B2 offer cheaper storage too. The plan will highlight at least S3 vs R2.
	•	File Metadata Display: Show basic info about the Parquet file on the site so users know what they are downloading. For example: “Dataset of AI/human story comparisons, in Parquet format. Size: 100 MB, updated Jan 2025.” This sets expectations (especially that Parquet isn’t directly human-readable like CSV, so possibly note “Parquet (use Python/R to open)”). This can be just a small text next to the download button.
	•	Priority II Features (Enhancements):
	•	Download Analytics and Limits: Track how many times the dataset is downloaded and possibly impose limits (if needed to prevent abuse). For example, one user can download at most X times per day, or we log each download event with user_id and timestamp. We could have an admin view to see “Dataset downloaded 50 times this week.” This is useful for project metrics or if offering multiple files. It could be as simple as incrementing a counter in the database when the API route is called (though a savvy user could call the S3 link multiple times within its short validity, but that’s minor).
	•	Alternate Data Formats: Perhaps offer the dataset in another format (CSV, JSON) if feasible, for users who cannot use Parquet. Parquet is great for big data, but some users might not know how to open it. If the dataset isn’t huge, a CSV might be easier for general audience. Alternatively, provide a small README with instructions on using Parquet (like link to Apache Parquet docs or a how-to). This could be a text on the site or an extra downloadable.
	•	Integration with Supabase Storage: Instead of directly using AWS SDK, we could leverage Supabase’s built-in Storage bucket. Supabase allows creating a bucket and managing files with RLS. We could store the Parquet in a private Supabase bucket. Then the user’s request could actually fetch a URL via Supabase’s API (supabase.storage.from('bucket').createSignedUrl('path', expiry)). This would simplify the auth story because Supabase can validate the user token and generate the signed URL for us. It’s an option if we want to keep everything in the Supabase ecosystem. The downside is we need to ensure the bucket can handle large file and bandwidth (Supabase uses S3 behind scenes; pricing might factor in).
	•	Cost Monitoring: For future-proofing, set up alerts on bandwidth usage if using a paid storage. If using AWS S3, maybe enable a budget or logging to know how much egress is happening. Or if using Cloudflare R2, monitor storage and operation counts (they charge per operation though no egress). For a moderately sized file, costs likely low, but as engagement grows it’s wise to keep an eye. This is more of an ops concern but part of planning.
	•	User Agreement or Authentication Tiers: If the data is sensitive or needs an agreement (some sites require agreeing to terms before download), consider adding a step where the user must confirm they won’t misuse it. Could be a simple checkbox “I agree to use this data for research purposes only” before enabling the download button. This could be implemented via a modal pop-up. It’s an extra feature if needed by policy. Also possibly restrict by user roles – e.g., only users who provided certain info or have a verified email can download. But unless specified, we assume all logged-in users can.
	•	Multi-file or Larger Dataset Handling: If in the future multiple datasets or versions are offered (say a new dataset each year), we might need an index page listing files. In that case, possibly implement a small list view with each file name, description, and download button. For now, just one file is assumed. But planning the code to handle a list of file entries could make it easier to extend.
	•	Implementation Plan:
	1.	Setup Storage and Obtain Credentials: For AWS S3: create a bucket (e.g. cwv-dataset) and upload the Parquet file there. Mark it private (no public read). Create an IAM user or role with permissions s3:GetObject on that bucket (and perhaps s3:ListBucket if needed) so that our server code can fetch or generate URLs. Securely store the AWS credentials (Access Key ID and Secret) in the Next.js app environment (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, plus the AWS_REGION and bucket name). For Cloudflare R2: similar steps (R2 uses S3-compatible API, but you’d use R2 access keys and endpoint). For Supabase Storage: ensure a bucket is made and note the API usage to sign URLs. In our plan, assume AWS for concrete steps.
	2.	Implement Backend Endpoint: In Next.js, create an API route pages/api/download-dataset.js (or TypeScript). This will do:
	•	Verify auth: possibly use Supabase admin client or decode a JWT. We might streamline by requiring the request to include a valid SupabaseAuthToken cookie or an Authorization: Bearer <token> header. Supabase provides a helper (supabase.auth.api.getUserByCookie if using their middleware) or we decode JWT using jsonwebtoken and Supabase public key. If verifying is too complex for MVP, as a shortcut, we could trust that the front-end only calls when logged in and double-check session in Supabase client (not ideal security, but mention it).
	•	If auth fails, return 401.
	•	If auth ok, initialize AWS SDK (for example, using @aws-sdk/s3-client). Use GetObjectCommand with expiresIn and get a presigned URL. In newer AWS SDK v3, we use S3RequestPresigner or simpler: use getSignedUrl(s3Client, new GetObjectCommand(params), { expiresIn: 60 }). params includes Bucket name and Key (file path).
	•	Return this URL in JSON, e.g. { "url": "https://...s3.amazonaws.com/bucket/obj?signature=..." }.
	•	(If using Supabase Storage instead, we would call supabase.storage.from('bucket').createSignedUrl('file.parquet', 60) and return that link).
	3.	Frontend Integration: On the front-end (maybe in a component on the homepage or a dedicated “Data” page), create a Download section. For example:

{user ? (
   <button onClick={handleDownload}>Download Dataset (Parquet)</button>
 ) : (
   <p><Link href="/login">Log in</Link> to download the dataset.</p>
 )}

The handleDownload function will call our API route:

const res = await fetch('/api/download-dataset', {
   headers: { 'Authorization': `Bearer ${supabaseToken}` }
});
if(res.ok){
   const { url } = await res.json();
   window.location.href = url;
} else {
   alert("Please log in to download.");
}

We pass the user’s token for auth (supabaseToken can be obtained via supabase.auth.session() on client). Alternatively, we could use cookies with Supabase Auth (it can set HttpOnly cookies if using the Next.js middleware, then our API could read those). If using that method, the API route could detect user via cookie automatically. In any case, upon receiving the URL, setting window.location to it triggers the browser to start downloading from S3. (The user experience: they click download, maybe a short delay while URL is fetched, then the browser’s save dialog appears or it downloads directly.)
Also consider handling error cases: e.g. if fetch returns 401, prompt login. If it returns some error (maybe file not found), show a message.

	4.	Cloud Provider Considerations: If we decide on Cloudflare R2 for cost reasons, integration is almost identical to S3 (since it’s S3 API compatible). We just have to use R2 endpoints. Cloudflare R2 has no egress fee ￼, which is great, but it might have slightly different latency or require setting up a custom domain for the bucket. Not a big issue. We could mention in config that switching to R2 would involve using R2 credentials and endpoint, but code remains same (using AWS SDK).
For GCP, the approach is similar but using Google Cloud client library or an S3-interoperability (GCS can generate signed URLs too). We likely stick to S3 semantics for simplicity.
	5.	Testing the Download Flow: Once implemented, test as a logged-in user: click download, ensure the file downloads and is not corrupted (verify file size and maybe attempt to open it with a Parquet tool). Also test as logged-out: the button should either not appear or should lead to a login prompt. Possibly test the security by trying to copy the signed URL and using it after expiry (should fail) or from another location. Also ensure the signed URL’s permissions are least-privilege (only GET allowed).
If using Supabase Storage approach, test that RLS is correctly preventing direct access (Supabase storage by default if not public, requires signed URLs or auth tokens).
	6.	UI/UX Enhancements: If the file is large, consider showing a little spinner or progress indicator when the user clicks download (since generating the signed URL is quick, but not instantaneous). In most cases it’s so fast that unnecessary, but on a slow server it might take a second or two. Could also simply change the button to “Preparing download…” while fetching.
Provide a small note that it’s Parquet format. Perhaps link to an explanation (“What is Parquet?” linking to an external doc) if we expect non-technical users. This could reduce confusion for those who might expect a CSV.
	7.	Cloud Cost Setup: As part of deployment, ensure environment variables (AWS keys etc.) are set in the hosting environment (e.g. Vercel or wherever Next is hosted). Also, possibly configure CORS on the S3 bucket to allow our domain if needed (for direct download it might not, since it’s a direct link and not an XHR). Actually, since the user’s browser will directly GET the S3 URL, CORS isn’t an issue for downloading a file (only if we were doing XHR to S3 from frontend, which we are not).
If using R2, ensure the R2 bucket has public read off (we rely on signed URLs). The R2 presigned URL approach is the same. We should mention how R2 would cut costs especially if dataset gets heavy use, as Cloudflare R2 charges $0 egress, directly challenging AWS’s pricing model for data out ￼.

	•	Recommended Cloud Storage Providers:
	•	Amazon S3: Industry-standard object storage. Pros: well-supported, easy SDK integration. Cons: Egress bandwidth can be costly (roughly $0.09 per GB after a free tier). For a moderately sized file and user base it’s manageable, but if this scales, costs rise.
	•	Google Cloud Storage: Similar to S3 in concept. Pros: seamless if our stack was on GCP, but since we use Supabase (Postgres on AWS) and possibly deploy on Vercel, sticking to AWS might be simpler. GCS egress costs are similar to S3.
	•	Cloudflare R2: Modern solution with no egress fees ￼. Pros: great for offering downloads to many users without worrying about bandwidth bills. Storage cost is around $0.015/GB which is low. It’s S3-compatible; we’d use the same code with a different endpoint URL. Cons: It’s relatively new, but by 2025 it’s mature enough. Could require setting up a custom domain or direct API access.
	•	Backblaze B2 or Wasabi: These are cheaper storage providers ($5/TB/month storage, free or cheap egress up to limits). They also have S3-compatible APIs (Wasabi does, B2 does via application keys). They can be considered for cost savings. The trade-off is sometimes slightly lower performance or less integrated tooling. But for a single file distribution, they’re fine.
	•	Supabase Storage: Since we already use Supabase, using its storage means one less external system. It has a straightforward JS API for signed URLs. Supabase storage pricing is similar to others (it’s built on S3 on the backend). If our dataset is not huge, this is a convenient route. The main difference is that Supabase limits file size depending on plan and we have to account for RLS rather than our own IAM logic.
	•	Conclusion: For planning, we suggest using AWS S3 initially (quick to implement with well-known patterns) and keep an option to migrate to Cloudflare R2 for cost efficiency if download volume is high. This way, we start with a known solution and can switch by updating config to point to R2 (which being S3-compatible, requires minimal code change, mostly endpoint and keys).
	•	Technical Considerations:
	•	Parquet Format Note: Parquet is a binary columnar format, not directly viewable by users without tools. We should assume users downloading it have the technical ability to use Python (pandas or pyarrow) or R to read it. Just to be user-friendly, maybe include a README or tooltip: “(Parquet format. You can use pandas or Apache Arrow to open.)” This prevents support questions. If the target users are mostly ML/AI researchers, they will know Parquet. If broader audience, consider also offering CSV as mentioned.
	•	File Size & Download Time: If the file is large (say hundreds of MBs or more), consider that some users on slow connections might have issues. Parquet compresses data well, but still. We could in future allow selective downloads (e.g. only certain parts of dataset) but that’s beyond scope. Perhaps just ensure our server doesn’t have a short timeout on the signed URL generation (should be quick anyway). The presigned URL will be used by the client’s browser to GET the file; that download could take time but that’s fine as long as the URL’s expiry is longer than the download time needed. If we set 1 minute expiry but a user’s download starts within that minute, it will continue even past expiry as long as it was initiated (S3 checks at start). To be safe, maybe set 5 or 10 minute expiry.
	•	Authentication Robustness: A savvy user could share the presigned URL with someone else, but it will expire soon. Also, we assume trust that users who we allowed to download are allowed to have the file anyway (so sharing might not be a big concern except for count). If it is a concern, we could tie the URL to an IP or use a short expiration. But usually short expiration is enough of a deterrent for mass sharing. Another approach: require login and track downloads so that each link is unique per user per request. But since the link is time-limited, that’s okay.
	•	Serverless Limitations: If hosting on serverless (Vercel functions), generating a presigned URL is lightweight and fine. But if we ever decided to stream the file through our Node server (not recommended), that could hit memory/time limits. So we definitely want the client to download directly from storage, not through our Next.js server.
	•	Supabase Integration: If we go with Supabase Storage in future, a nice benefit is we could leverage Supabase’s auth directly: each user could fetch the file with their Supabase JWT without an extra signing step if we configured RLS to allow it. But simpler is still presigned approach (Supabase has createSignedUrl). Both approaches are similar from user perspective.
	•	UI Feedback: After download, perhaps ask for feedback or show a “Thank you for downloading” or link to any documentation for the dataset. This is not necessary but a friendly touch. Could even use that moment to remind users if they use the data in a paper to cite our project, etc. This could be just a static message on the download section or a small pop-up.

5. User Authentication and Metadata Collection

Overview: The platform will use Supabase for user authentication (as it’s already integrated) to handle sign-ups and logins. We will implement a full user account system including sign-up, email login (and possibly OAuth if needed), and ensure a smooth sign-in flow. In addition, we want to collect some demographic and interest metadata about users, specifically age, educational status, and interest in literature. This information will be gathered at onboarding (first login or registration) and stored in the database, linked to the user. Furthermore, all user-generated data (comparisons, highlights, etc.) should be associated with the user’s profile so that we can later analyze correlations (for example, do literature enthusiasts detect AI texts more easily, etc.). Supabase will serve both as the authentication provider and the database to store user profiles and annotations, making it convenient to enforce security (using row-level security so users only access their own data).
	•	MVP Scope: Implement user authentication via Supabase such that users can sign up with email and password, and then sign in to the platform. Use Supabase Auth’s built-in methods for this. Once logged in, the user should be prompted exactly once (on first sign-up or first time visiting their profile) to provide their age, education level, and interest in literature. This could be a short form asking, for example: age (as a number or range), education (perhaps dropdown: high school/undergrad/graduate/other), and a self-rated interest in literature (could be a scale or options like “Casual reader / Avid reader / Literature student / etc.”). This metadata is then saved in a User Profile table in the Supabase database, linked to the user’s UID from Auth. For any annotation actions (like the comparisons, highlights from earlier features), ensure that we record the user’s ID with those records. The MVP should enforce login for key actions (like downloading data, perhaps doing comparisons if we want all data linked – or we allow browsing comparisons as guest but prompt login to record results).
	•	Priority I Features (Must-Haves):
	•	Sign-Up & Sign-In Flows: Users should be able to create an account and then log in. With Supabase, we can leverage either the Supabase JavaScript SDK or an Auth UI component. Likely we’ll create custom forms to better control the experience. A sign-up form will collect email, password, and perhaps directly the age/education interest fields (to reduce steps). Alternatively, keep sign-up simple (email & password), then on first login, present the profile form for age/etc. We will implement both flows: one for new registration (with email verification if possible), and one for returning users (login form). Supabase Auth handles password hashing and verification behind the scenes.
	•	Supabase Integration: Use the Supabase JS client in our Next.js app. We will initialize the client with our Supabase URL and public anon key (these come from Supabase project settings). We’ll likely configure Supabase Auth to send confirmation emails if that’s desired (Supabase can send a verify email or we can disable that for simplicity). For MVP, possibly disable email confirmation to reduce friction, but ensure we have a way to prevent spam. It’s fine either way; if academic setting, maybe we allow unverified logins initially to get data quickly.
	•	Profile Metadata Collection: Design a Profile table in the Supabase Postgres. For example:

create table profiles (
  id uuid primary key references auth.users(id),
  age int,
  education text,
  literature_interest text,
  created_at timestamp default now()
);

This table uses the user’s UUID (from Supabase Auth) as the primary key and foreign key to auth.users. We will store age (could be int or smallint), education (could be an enum or text like “HighSchool/College/Masters/PhD/Other”), and interest (text or an integer scale 1-5 representing interest level, or categories like “Not much / Some / A lot”). We can refine those fields but keep them simple. When a new user signs up, we need to insert a row into this profiles table. We can do this in two ways:
	•	Use a database trigger that on auth.users table insert, creates a new profile row. Supabase even provides such examples, or a quick SQL trigger can set default values (null or placeholders).
	•	Or handle it in application code: after successful sign-up, call a Supabase function to create the profile entry with the collected metadata.
We might go with application code for flexibility (so we can include user-provided fields). Either way, ensure that every user has a corresponding profile row. Supabase’s docs often show this approach ￼: the app authenticates the user and stores profile info in the database (with RLS to secure it).

	•	Onboarding Form UX: After sign-up (or on first login), present the user with a form or modal to fill in age, education, interest. This should be required to proceed (or skippable if we allow skipping, but the prompt says to collect it, likely we want it). Make the UI simple: for age maybe a number field or dropdown of ranges (to avoid overly precise data if privacy concerned, but we can just take exact age or birth year). Education level: a dropdown (options: High School, Bachelor’s, Master’s, PhD, Other). Interest in literature: perhaps a dropdown or radio: e.g. “Not much interest / Some interest / Very interested / Literature background” or a 1-5 scale with labels. We should decide discrete categories for easier analysis later. For example: Interest in literature: Low, Medium, High could suffice. The UI collects these and on submission, saves to the profile table (via Supabase insert or upsert).
	•	Linking Annotations to User: All data from features 1 and 2 (comparisons, highlights, reasons) should include a user identifier. If using Supabase, typically the user’s UUID is accessible on the client via supabase.auth.user().id. We will include that when inserting records. Alternatively, if we use Supabase Row Level Security, we can use the policy to automatically assign user_id from the JWT. For instance, in the comparisons table, have a column user_id uuid references auth.users. We can create a policy: INSERT: user_id = auth.uid() to allow users to only insert their own id. If using Supabase client on the frontend, we can rely on RLS to attach the right user id (with a function or using auth.uid() in a DEFAULT value or trigger). A simple approach is just pass the user_id in the insert call explicitly (since the client has it) – but ensure RLS policy allows it only if matches auth.uid() to avoid spoofing.
The main point: whenever we store user’s choice or highlight, it’s tied to their unique ID. This way, in analysis, we can join with their profile info.
	•	Authentication Guarding: Determine which parts of the site require login. Likely: downloading the dataset (as per feature 4) requires login. Perhaps contributing to comparisons (feature 1 & 2) should also require login to get higher quality data and associate it. However, we might allow a curious visitor to try the comparison without logging in (in which case their data would be anonymous or not saved). But since we want metadata link, better to encourage login. We could do a compromise: allow viewing the two samples as a demo but to submit your vote and see the answer, prompt login. This can be a growth tactic to convert users. But the safer assumption is we require login for full participation. So implement checks: if user not logged in and tries to access main interactive components, redirect to login or show a modal. Next.js has middleware support to protect routes or we can conditionally render content.
In code, for example, if on the comparison page, do:

const { user } = useSupabaseAuth(); 
if (!user) { return <PleaseLoginPrompt/>; }
else { return <ComparisonUI/>; }

A PleaseLoginPrompt could be a nice message with a link to sign up/in.

	•	Supabase Row-Level Security (RLS): Enable RLS on the profiles table and any user-data tables. Then create policies such that each user can SELECT and UPDATE their own profile, and insert if none exists. For example, profile table policy:
	•	Select policy: auth.uid() = id (only see your profile)
	•	Update policy: auth.uid() = id (only update your profile)
	•	Perhaps no delete allowed for users (to prevent deleting profile; or allow if you want user to remove account info).
For comparisons or annotations table:
	•	Select policy could allow maybe only user’s own or maybe none (if data is sensitive; but admin might export all later outside RLS). Possibly we don’t even need normal users to read all their past comparisons from the API, unless we show history. We might not have a user-facing history page at MVP, so could just allow insert and maybe select only their own if needed.
	•	Insert policy: allow if user_id = auth.uid(). This way, even if a malicious user tried to insert on behalf of another user_id, it would be blocked - ensuring integrity.
	•	We can have Supabase function default the user_id as auth.uid() on insert using a stored procedure or just let client specify but with policy check.
These measures secure user data. Supabase’s example app uses similar RLS for profiles ￼.
	•	Account Management: Provide basic account management features such as logout button, and maybe a simple profile view where they can edit their metadata if needed. Editing might be nice (if someone made a mistake in age or so). So allow the user to navigate to a “Profile” page: show their current info and allow changes. This would just be an update query on the profiles table. Ensure to guard it with RLS as above.
For password reset or email change, Supabase has built-ins (reset emails, etc.). At least implement a logout (supabase.auth.signOut()). Possibly also a “forgot password” link on login form that triggers Supabase’s password reset email (this can be done by supabase.auth.api.resetPasswordForEmail if we integrate that).
Since the question doesn’t explicitly ask, we can consider password reset a standard expectation but not focus heavily in writing.

	•	Priority II Features (Enhancements):
	•	Social or SSO Logins: Supabase Auth supports OAuth providers (Google, GitHub, etc.). As an enhancement, we could allow users to sign in with Google or other providers to lower friction. This would still yield a Supabase user ID and we can create a profile for them as well. It simplifies not having to manage passwords. For now, email/pass is fine, but in future adding “Login with Google” could improve sign-up rate.
	•	Email Verification & Communication: Enable email confirmation to ensure users sign up with real emails. Supabase can send a confirmation link. Also, we might want to send a welcome email or follow-ups (this would require setting up SMTP in Supabase settings). Not required but an enhancement for polish. Also, if we intend to gather research data, having verified emails might be less important than just getting many users, but it’s a thought.
	•	Additional Profile Fields: We could collect more metadata if beneficial – e.g. gender, native language, etc., if those might influence results. But since only age, education, interest were requested, we stick to those. If in future the research needs more, the profile can be extended. So design the profile table and code to be extendable (adding new columns easily).
	•	User Dashboard: Provide a dashboard page where users can see some stats about their activity: number of comparisons they’ve done, their score, maybe a summary of their preferences. This not only engages them but also demonstrates we link their data. For example, “You have completed 20 comparisons. Your accuracy in guessing AI is 60%. Keep it up!” and possibly “You highlighted X different sentences.” This uses the stored data linked by user_id. This is a nice-to-have feature that uses the metadata and data collected to give personalized feedback. It could increase retention as users may want to improve their stats.
	•	Admin Tools: Eventually, an admin might want to view all results or download all user data for analysis. We could create an admin role and an admin-only page that displays aggregated data or allows exporting of the entire comparisons table, etc. This would use either a Supabase service_role key (bypassing RLS) or a separate privileged Supabase instance. But in planning, just note that data is easily accessible via the DB for research analysis (maybe directly connecting to Supabase or using the Parquet if that’s an export of it).
	•	Privacy and Opt-In: Because we’re collecting personal info (age, edu), we should be transparent to users. Possibly include a privacy note on the sign-up or profile form: “This info will be used for research analysis but will be kept confidential.” And maybe allow “prefer not to say” for any field if someone is uncomfortable (particularly age or edu). But since it’s research, we might want to enforce it. Alternatively, allow skip but mark profile with a flag that metadata is incomplete (which could exclude them from certain analyses).
	•	Interest-based Personalization: If a user indicates high interest in literature, we could tailor content to be more challenging or show them additional info (like maybe they’d want to read the full paper). If low interest, maybe we keep things more gamified and not overly academic. This is speculative, but we can use the interest field to segment user experience. For example, maybe send literature enthusiasts a prompt or invitation to longer surveys. However, implementing actual divergent UX might be too much – but even simple personalization like a different welcome message (“Welcome literature lover!” vs “Welcome!”) could be a cute touch. This would be a subtle enhancement.
	•	Implementation Plan:
	1.	Integrate Supabase Auth in Next.js: If not already done by the repo, set up the Supabase client. In Next.js, we might have a context provider for Supabase or use the hooks provided by @supabase/auth-helpers-nextjs. Supabase offers an official auth-helpers package that simplifies managing the user session in Next (with support for Next 13 App Router too). We can use that or manually handle it. A straightforward way: initialize Supabase client with anon key, then use supabase.auth.signUp({ email, password }) and supabase.auth.signIn({ email, password }) for forms. Also, configure the callback URL if using magic links, etc.
	2.	Build Auth UI Pages: Create pages/signup.jsx and pages/login.jsx (or combine into one component with toggling). The sign-up form asks for email, password, and possibly directly asks for age, edu, interest. There’s a trade-off: asking extra info at sign-up can deter some people. Perhaps we ask after they verify email or after initial sign-up. To maximize sign-ups, perhaps only ask email & password, then on the next screen ask profile questions. That next screen could be the “onboarding profile” page. Let’s do that: keep sign-up minimal (maybe just email & password). After supabase.auth.signUp returns success (if email confirmation off, they’re logged in immediately; if on, we need to handle verification email sent), we navigate to a profile completion page.
	3.	Profile Completion Form: Create pages/onboarding.jsx which is protected (only accessible if logged in, and possibly if they haven’t submitted profile info yet). This page will have fields for age, education, interest. On submit, we call supabase.from('profiles').upsert({ id: user.id, age:..., education:..., literature_interest:... }). Alternatively, since the user might not yet have a profile row (depending on trigger usage), use insert. If using an INSERT trigger to auto-create an empty profile on sign-up, then we should use update instead. Using upsert covers both cases. After saving, redirect user to home or wherever.
	4.	Enforce one-time collection: We should ensure we don’t ask these every login. Once profile is filled, maybe store a flag in local or just check the presence of data in their profile. For instance, the profiles table could have these fields default null. We can consider the profile “complete” if none are null. If not complete, redirect to onboarding form. This can be done by a check on app load or within protected routes. For example, when a user logs in and we fetch their profile (via supabase select), if any crucial field is null, we navigate to /onboarding. This way, they are forced to fill it to proceed. Also, provide a logout option if they really don’t want to.
	5.	Storing and Linking Data: As we implement features 1 & 2, ensure that when creating records like comparisons, we include user_id: user.id. If using Supabase client, we might rely on RLS and have a user_id default. Another pattern: not use Supabase client for inserts, instead have our Next.js API do it (but since Supabase is already on frontend for realtime updates etc., directly from client is fine). For simplicity, use supabase client on frontend to store comparisons and highlights as they happen. This requires that the user is logged in (so they have a valid supabase session with the JWT that RLS will use to allow inserts). That’s fine because only logged-in can do it anyway.
	6.	Logout and Session Handling: Implement a logout button (maybe in a menu or top-right account icon). On click, call supabase.auth.signOut(), which clears session. Also redirect to homepage or login page. Also handle session persistence – Supabase by default keeps a logged in session in localStorage and refreshes tokens. We might use Supabase’s <SessionContextProvider> to manage this easily (especially in Next 13). It will make user available via hook useUser(). We should utilize these helpers to avoid manual token management.
	7.	Testing Auth Flows: Test creating a new user: after sign-up, ensure profile form appears and data is saved. Check in database that profile row is created and populated. Test login with an existing user: ensure it doesn’t ask profile again (maybe fetch profile and see data is there). Also test log out -> log in again. Test edge cases: sign-up with an email that already exists (Supabase will error; handle by showing user a message). Wrong password on login (show error). Possibly test the password reset: we could add a “Forgot password?” link that calls supabase.auth.resetPasswordForEmail(email) which triggers an email (for MVP, maybe skip, but it’s not hard to include if email setup is done).
	8.	Security & RLS: After implementing, double-check that RLS policies are properly set so users cannot see others’ profiles or data. For instance, try to query another user’s profile via Supabase client (should be forbidden). Ensure that our supabase queries in client specify the appropriate filters (like supabase.from('profiles').select('*') will by RLS only return that user’s row anyway if policy is set). It’s often wise to explicitly filter by user_id even with RLS, e.g. supabase.from('comparisons').select('*').eq('user_id', user.id) to avoid accidentally retrieving nothing or everything if misconfigured.
	9.	Metadata Usage: Though not needed for functionality, verify that linking works by doing a sample query in the database: join a comparison row with profile table on user_id and see that data lines up. This will be how analysis is done (maybe outside the app or via admin queries). Essentially, by implementing this linking, we ensure we can later run SQL like SELECT age, education, count(*) FROM comparisons JOIN profiles ON comparisons.user_id=profiles.id GROUP BY age, education to see trends. This is the end goal of collecting metadata.
	•	Dependencies / Tools:
	•	Supabase JS SDK: Already in use for auth and DB. Link: Supabase JavaScript library. We use it for all auth (signUp, signIn, signOut) and database operations (select/insert/update on profile and data tables).
	•	Supabase Auth Helpers (optional): Provides React hooks and server-side helpers for Next.js. For example, createBrowserSupabaseClient and <SessionContextProvider> to manage session across pages, and maybe middleware to protect pages. Link: Supabase Auth Helpers for Next.js. This could simplify protecting routes (there’s a middleware that redirects if no session). If comfortable, we integrate it. Otherwise, manage via client checks.
	•	Next.js API (optional): If we prefer to handle profile creation via a serverless function (to hide logic or use service role), we could create an API endpoint that the onboarding form calls. That endpoint, using service role key, could insert profile. But that’s not necessary if we trust client with RLS. Possibly skip to reduce complexity.
	•	Forms and Validation: We might use a library like React Hook Form for managing form state and validation (e.g. ensure age is a number between plausible range, etc.). Or simple useState and basic checks is fine. For user experience, adding some validation messages (like “Please enter a valid email” or “Age must be a number”) is good.
	•	Encryption: Probably not needed for these fields, but if any field was sensitive, we could encrypt or hash. Age and education are not highly sensitive (though still personal). We rely on restricting access via RLS. If extra cautious about PII, one could hash emails, but Supabase Auth stores emails anyway for login. So, we lean on Supabase’s security.
	•	Technical Considerations:
	•	Data Privacy: We must ensure the personal data (age, etc.) is protected. Since only the user and perhaps admins can see it (with RLS in place), we fulfill that. If we ever expose user info (like a leaderboard), avoid showing personal metadata publicly. The system as planned keeps it internal.
	•	Scalability: The amount of user data (just a few fields per user) is trivial to handle. Supabase can easily manage that. One thing to watch: by linking every action to user, queries for analysis might need to join across potentially many rows if we get huge participation. That’s fine for Postgres if indexed by user_id. We should ensure to index user_id in comparisons/annotations table for quick joins by user or filtering by user. Supabase likely encourages that structure anyway.
	•	Session Management: Supabase issues JWTs (usually expiring in 1 hour by default, with auto-refresh). The auth-helpers can handle refresh seamlessly. We should integrate that so that a user’s session stays active as they use the site, without needing to re-login frequently. If using the provider, it will refresh tokens under the hood. If not, we might need to call supabase.auth.getSession() periodically. But better to let their library handle it.
	•	Multi-Tab and SSR: If using Next SSR and Supabase, ensure we don’t expose the service_role key (never include that in client). We likely only use the anon key on client. For SSR (if we needed to fetch user data on server side), we’d pass a supabase session token via cookies. The auth-helpers-nextjs provide a middleware that sets a cookie sb-access-token. We might adopt that for convenience. It’s an advanced detail; MVP can be purely client-side rendered after login.
	•	Email deliverability: If enabling verification or password resets, configure SMTP settings in Supabase (SendGrid or similar). In testing, this should be done. If skipping verification, be aware of spam signups. Possibly implement reCAPTCHA if worried about bots, but likely not needed for a research project unless open to the world and heavily promoted.
	•	Linking to Download: As part of authentication, ensure the download check uses the Supabase session. Possibly use supabase.auth.getUser() on the server or decode JWT as earlier, to allow only logged-in downloads. We have covered that in section 4.
	•	Future Account Features: If needed, users might want to delete their account/data. Not asked, but ethically we might consider a deletion option. That would involve removing their entries (or marking them anonymous). Could plan if needed, but out of current scope.
	•	Consistency: Use the same user ID (Supabase’s) across all tables (which we do). The Supabase auth.users table holds base info (email, etc.). We might not expose those in our UI except email possibly in a profile page (“logged in as email@example.com”). If we do show email, fetch it via user.email from the auth object (no need to duplicate in profiles).
	•	Example from Supabase tutorial: Notably, Supabase’s Next.js example app does what we want – it authenticates, then stores profile info (like username, avatar) in a table, with RLS. We are basically following that pattern, just with different fields. So we can be confident this approach is well-tested.

In conclusion, by implementing a solid authentication system with Supabase and collecting user metadata upon onboarding, we will be able to personalize the experience and link user traits to their interaction data. This is crucial for analyzing the results of the human vs model comparisons in a research context. It also enhances user engagement, as they can have accounts, see their stats, and trust that their data is secure.

## Notes on Post-MVP Implementation
- The plan calls for an interactive PDF viewer using `react-pdf`. This library is not installed in the repo and cannot be fetched in the sandbox. To implement: add `react-pdf` and `pdfjs-dist` as dependencies, dynamically import a `PDFViewer` component that uses `<Document>` and `<Page>` with custom zoom and page navigation controls. Configure `pdfjs.GlobalWorkerOptions.workerSrc` to point to the bundled worker in `/public`.
- Dataset download analytics are mentioned but no table schema exists. One approach: create a `dataset_downloads` table with columns `id`, `user_id`, and `downloaded_at` (timestamp). Update `/api/download-dataset` to insert a row when a signed URL is generated.
- A new human-vs-model mode will use OpenAI API to generate text. Clarify which table stores human stories and create a table like `human_model_evaluations` to log guesses and model choice. Implement API route to call OpenAI and return generated text without exposing the key.

